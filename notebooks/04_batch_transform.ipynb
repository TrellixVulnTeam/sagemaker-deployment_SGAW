{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U pandas pandas-profiling scikit-learn sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the inference script\n",
    "- Similar to real time inference, first we need to write the 4 functions for model inference in a .py script\n",
    "- Sagemaker API documentation: https://sagemaker.readthedocs.io/en/stable/api/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serve.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load and return the model\"\"\"\n",
    "    model_file_name = \"pipeline_model.joblib\"\n",
    "    pipeline_model = joblib.load(os.path.join(model_dir, model_file_name))\n",
    "    \n",
    "    return pipeline_model\n",
    "      \n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Process the input json data and return the processed data.\n",
    "    You can also add any input data pre-processing in this function\n",
    "    \"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        input_object = pd.read_json(request_body, lines=True)\n",
    "        \n",
    "        return input_object\n",
    "    else:\n",
    "        raise ValueError(\"Only application/json content type supported!\")        \n",
    "\n",
    "def predict_fn(input_object, pipeline_model):\n",
    "    \"\"\"Make predictions on processed input data\"\"\"\n",
    "    predictions = pipeline_model.predict(input_object)\n",
    "    pred_probs = pipeline_model.predict_proba(input_object)\n",
    "    \n",
    "    prediction_object = pd.DataFrame(\n",
    "        {\n",
    "            \"prediction\": predictions.tolist(),\n",
    "            \"pred_prob_class0\": pred_probs[:, 0].tolist(),\n",
    "            \"pred_prob_class1\": pred_probs[:, 1].tolist()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return prediction_object\n",
    "\n",
    "def output_fn(prediction_object, request_content_type):\n",
    "    \"\"\"Post process the predictions and return as json\"\"\"\n",
    "    return_object = prediction_object.to_json(orient=\"records\", lines=True)\n",
    "    \n",
    "    return return_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger Batch Transfrom Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import Session, get_execution_role\n",
    "\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "training_job_name = \"...\" # TODO: Update with best TrainingJobName from hyperparameter tuning\n",
    "model_artifact = f\"s3://{bucket}/{training_job_name}/output/model.tar.gz\"\n",
    "endpoint_name = \"heart-disease-rfc-pipeline-batch-transform\"\n",
    "\n",
    "base_model = SKLearnModel(\n",
    "    name=endpoint_name,\n",
    "    framework_version=\"1.0-1\",\n",
    "    entry_point=\"serve.py\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    model_data=model_artifact,\n",
    "    role=get_execution_role(),\n",
    "    sagemaker_session = session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW! Create a batch transformer from the base model\n",
    "output_path = f\"s3://{bucket}/sagemaker/heart_disease/test_preds\"\n",
    "batch_transformer = base_model.transformer(instance_count=2, \n",
    "                                           instance_type=\"ml.m5.large\",\n",
    "                                           strategy=\"MultiRecord\",\n",
    "                                           accept=\"application/json\",\n",
    "                                           assemble_with=\"Line\", \n",
    "                                           output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feed the test data\n",
    "test_data_path = \"...\"  # TODO: Paste the S3 path to your bigtest.json\n",
    "batch_transformer.transform(test_data_path, content_type=\"application/json\", split_type=\"Line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output path\n",
    "output_path = f\"{batch_transformer.output_path}/bigtest.json.out\"\n",
    "print(\"Output written to: \")\n",
    "print(f\"{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_path = f\"{batch_transformer.output_path}/bigtest.json.out\"\n",
    "preds_df = pd.read_json(output_path, lines=True)\n",
    "\n",
    "print(preds_df.shape)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join predictions to input\n",
    "bigtest = \"../data/bigtest.json\"\n",
    "bigtest_df = pd.read_json(bigtest, lines=True)\n",
    "\n",
    "bigtest_df = bigtest_df.join(preds_df)\n",
    "bigtest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test accuracy\n",
    "len(bigtest_df[bigtest_df[\"target\"]==bigtest_df[\"prediction\"]])/len(bigtest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sm_client.delete_model(ModelName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
